{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3db45f-c640-4612-b50c-9d0b9682413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import langid\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from morfeusz2 import Morfeusz\n",
    "from advertools import url_to_df \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "requests.packages.urllib3.disable_warnings() # We will rised by failed url requests, which won't be taken into account either way\n",
    "\n",
    "BROWSING_HISTORY_JSON = 'data/BrowserHistory.json'\n",
    "WORDBAGS_JSON = 'data/BrowsingHistoryWordbags.json'\n",
    "CHROMEDRIVER_EXEC = r'..\\LocalPathVariables\\Chromedriver_bin\\chromedriver.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9759f83d-b153-4b9e-b6d9-077ccfd92e02",
   "metadata": {},
   "source": [
    "## Importing the browsing history\n",
    "\n",
    "We'll import the data spanning from 19.10.2021 untill 19.03.2022, with the latter marking the beginning of this project.\n",
    "\n",
    "After reading the data into a dataframe we're limiting it to chosen timespan and accessable URL's (beginning with \"http\" / \"https\"). We're also leaving the columns of interest, it is the title, url and time_usec.\n",
    "\n",
    "We then reset the indexing to match the current record count. Zero will be the first index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5d3621-57af-4060-8274-a128df0fb860",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(BROWSING_HISTORY_JSON, encoding='utf-8'))\n",
    "df = pd.json_normalize(data, record_path=['Browser History'])\n",
    "df.drop(df[df.time_usec < 1634601600000000].index, inplace=True)\n",
    "df.drop(df[~df.url.str.contains('http://') & ~df.url.str.contains('https://')].index, inplace=True)\n",
    "df = df[['title', 'url', 'time_usec']]\n",
    "df.index = [i for i in range(0, len(df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2117127e-a1a0-4c32-a52a-4fe6db55e8b3",
   "metadata": {},
   "source": [
    "## Partitioning of URL's\n",
    "\n",
    "As we want to separate the domain form the whole address we use advertools.url_to_df() function, which creates a dataframe object where each row gets an entry for a specific url seciton. We'll only use the netloc field, as it's the domain we look after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5df45c-9c64-4113-b4a5-cb3009766420",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add = url_to_df(df.url)[['netloc']]\n",
    "df = df.join(df_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47da85f2-b840-4701-823f-ad5f37f282df",
   "metadata": {},
   "source": [
    "## Volume reduction\n",
    "\n",
    "In this section we reduce the overall volume of our dataset for the purpose of noise cutting. We're implementing the solutions derived in the README file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a4b63-2f5d-41e6-acee-6f084add2097",
   "metadata": {},
   "source": [
    "### Limiting repetitions\n",
    "\n",
    "We're cutting short the lurking peroids, thus getting closer to representing the actual \"entrances\". I also decided to treat all google search queries as noise. I'd agree with the view that it may be bold of me to think that a great deal of potential information isn't lost this way, but I justify it by saing that even if we traet each query as a separate entrance (instead of bucketing under google.com domain, which wouldn't be informational due to a large variety of searched topics) each included page may greately broaden the wordspace we'll have to choose features for, making the similarity matrix even more sparse.\n",
    "\n",
    "Later we drop marked rows along with the temporary \"del\" column.\n",
    "\n",
    "It's also worth to create a table consisting of browsing history share per domain before removing repetitions for the sake of later analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f54d82f-3994-45e3-8772-3c29269a7f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amounts = df['netloc'].value_counts()\n",
    "df['del'] = [False if (df.netloc[i] == df.netloc[i+1] or df.netloc[i] == 'www.google.com') else True\n",
    "             for i in range(0, len(df.values)-1)] + [True]\n",
    "df = df[df['del']][['time_usec', 'title', 'url', 'netloc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c7122f-bcb0-4873-8b9c-ac188632ccd3",
   "metadata": {},
   "source": [
    "### Reduction of outliers\n",
    "\n",
    "I decided to drop all entries about sites that I've visited not more than 23 times during the last 6 months. I'd be hard to speak about a routine if I didn't visit a site at least 5 times per month, so it'd be reasonable to cut it off here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e4101-ccb4-482c-a323-0991471d6ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['netloc'].map(df['netloc'].value_counts()) > 23]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18061d3d-7656-4bed-91c2-d4c7db505416",
   "metadata": {},
   "source": [
    "## Downloading\n",
    "\n",
    "Buckets dataframe is created to keep just two types of information: the domain name and a wordbag field, to which we will append the trimmed and prepared contents of its subdirectiories that I've visited. In order to cut down on the amount of iterations we create a set called history which will store full URL's of sites whose content has been already downloaded.\n",
    "\n",
    "To keep it even simpler we only allow ourselves to include sites whose response code is equal to 200 (meaning connection without exceptions). It later tourns out that few sites, ylilauta.org to give an example, is protected against webcrawling bots, so we'll have to label them manually.\n",
    "\n",
    "The process of stripping the text has three steps. Firstly we strip it out of html tags, secondly we analise each character to keep only normal letters.\n",
    "Secondly each word is getting checked for appearance of uppercase letters inside of it.\n",
    "There's a large probability that those would be leftovers of variable names from code. Additionally we specify to keep them in the length range between 4 and 20 characters, lowercasing them all afterwards.\n",
    "It's all finished by stemming with the use of two lemmatisation libraries. Morfeusz() to be used for polish and a PorterStemmer for english, usage of which is regulated by classify() function from langid library applied to first 20 words from the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef53533-aadf-4bf3-9337-c2f58cb9069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buckets = pd.DataFrame({'netloc' : df['netloc'].unique(),\n",
    "                           'wordbag': ''})\n",
    "history = set()\n",
    "\n",
    "op = webdriver.ChromeOptions()\n",
    "op.add_argument('headless')\n",
    "driver = webdriver.Chrome(executable_path=CHROMEDRIVER_EXEC, options=op)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    \n",
    "    if row['url'] not in history:\n",
    "        \n",
    "        sc = 0\n",
    "        history.add(row['url'])\n",
    "\n",
    "        try:\n",
    "            page = requests.get(row['url'], verify=False)\n",
    "            sc = page.status_code\n",
    "        except: \n",
    "            pass\n",
    "        if sc == 200:\n",
    "            print(f'Conducting {row.url} ...') # The script is verbose. \n",
    "            \n",
    "            driver.get(row['url'])\n",
    "            for _ in range(10):\n",
    "                sleep(0.35)\n",
    "                driver.execute_script('return scrollBy(0, 400);')\n",
    "\n",
    "            soup =  BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            text = ''.join(i for i in soup.stripped_strings)\n",
    "\n",
    "            words_lst = ''.join(e if (e.isalnum() or e == ' ') and not e.isdigit() else ' ' #Analiza znakÃ³w\n",
    "                                for e in text).split() \n",
    "            words_lst = [e.lower() for e in words_lst\n",
    "                            if not any([bool(re.match(r'.\\w*[A-Z]\\w*', e)), len(e)>20, len(e)<4])] \n",
    "\n",
    "            if langid.classify(' '.join(words_lst[:20]))[0] == 'pl':\n",
    "                stemmer = Morfeusz()\n",
    "                text = ' '.join(next(iter(stemmer.analyse(e)))[2][1].split(':')[0] for e in words_lst if not e in stopwords.words('polish')) \n",
    "            else:\n",
    "                stemmer = PorterStemmer()\n",
    "                text = ' '.join(stemmer.stem(e) for e in words_lst if not e in stopwords.words('english')) #Stemming\n",
    "\n",
    "            if row['netloc'] in df_buckets['netloc'].values:\n",
    "                df_buckets.loc[df_buckets['netloc'] == row['netloc'], 'wordbag'] += f' {text} '\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7c38eb-5f24-4d17-b5a7-8c61bb93d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4800e290-5603-4ffd-906a-65e1e1d46426",
   "metadata": {},
   "source": [
    "### Saving\n",
    "\n",
    "We save it in json format for use in separate script for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f60f1-1bf8-458d-9e18-6789a86c3ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buckets.to_json(WORDBAGS_JSON)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
