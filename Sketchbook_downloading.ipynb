{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3db45f-c640-4612-b50c-9d0b9682413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import langid\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from morfeusz2 import Morfeusz\n",
    "from advertools import url_to_df \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "\n",
    "BROWSING_HISTORY_JSON = 'data/BrowserHistory.json'\n",
    "WORDBAGS_JSON = 'data/BrowsingHistoryWordbags.json'\n",
    "CHROMEDRIVER_EXEC = r'C:\\Users\\Szymon\\Documents\\Kod\\LocalPathVariables\\Chromedriver_bin\\chromedriver.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a3bf0-fc26-4c46-9497-ada2735f74d3",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "The goal is to analise the timeseries arising from my web browser history in order to predict my usual daily routine. Along the way we'll make use of a clustering algorithm for the purpose of automatically labeling the visited sites as laisure-oriented (for example: youtube, facebook or coub), math- or programming- oriented (stack overflows, various documentations, wikipedia) etc. \n",
    "\n",
    "This posts a demand for two types of data: the site's contents as a document and a timeseries of visited site types with accordance to timestamps. Proper gathering and preparation of both of them will be challanging.\n",
    "\n",
    "First off with the sites. As we'll feed it to a clustering alghoritm via some document vectorisation method, we need consider the following:\n",
    "\n",
    "1. The less sparse the similarity matrix (made up of documents' feature vectors) the better the clustering algorithm will perform, and as each feature vector entrance is associated with one word (globally, since every document's feature vector has to \"make a spot\" for its own value for that word), we should choose only the most important of them. Our sense measure of importance will be dictated by:\n",
    "    - The need to represent site's visible content, not the structure. We need to get rid of any code contained in downloaded .html file.\n",
    "    - Our decision not to take context into account (for simplicity, as we feel confident that a general document clustering will do), so we don't need to include interpunction or special characters in general.\n",
    "    - The need to reduce noise inside the document. We'll filter for stopwords.\n",
    "    - Generality of a word. We should count different forms of one word as several occurences of one word. We will stem each word.\n",
    "\n",
    "2. Downloading should be performed in a way optimising the end-goal timeseries, so:\n",
    "    - We need to get rid of noise in the browsing history. We should omit sites that don't meet a visit-frequency threshold.\n",
    "    - We should get rid of repetitions. We can do it in two ways:\n",
    "        + We can create \"domain buckets\" that represent a domain with its various paths as one entrance (to be explained in details later) along with a general content profile of each. It serves not only to reduce the download, but to make it more stable in clustering process.\n",
    "        + By skipping the lenghty lurking process on one domain bucket and leave just one representative, later to fill the gap between timestamps with that copies of it.\n",
    "    - We need to drop records of sites that we cannot access.\n",
    "    \n",
    "After such processing, we should achieve a nice, dense similarity matrix.\n",
    "\n",
    "Additionally we'll make a comparison with a clustering based on pages' titles. Their preparation will be same as for the sites the scope of the second point above, but word filtering and generalisation won't be so restrictive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9759f83d-b153-4b9e-b6d9-077ccfd92e02",
   "metadata": {},
   "source": [
    "## Importing the browsing history\n",
    "\n",
    "We'll import the data spanning from 19.10.2021 untill (but not exactly) 19.03.2022, which marks the beginning of this project.\n",
    "\n",
    "After reading the data into a dataframe we're limiting it to chosen timespan and accessable URL's (beginning with \"http\" / \"https\"). We're also leaving the columns of interest, it is the title, url and time_usec.\n",
    "\n",
    "We then reset the indexing to match the current record count. It's worth noting that we begin at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff5d3621-57af-4060-8274-a128df0fb860",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(BROWSING_HISTORY_JSON, encoding=\"utf-8\"))\n",
    "df = pd.json_normalize(data, record_path=['Browser History'])\n",
    "df.drop(df[df.time_usec < 1634601600000000].index, inplace=True)\n",
    "df.drop(df[~df.url.str.contains('http://') & ~df.url.str.contains('https://')].index, inplace=True)\n",
    "df = df[['title', 'url', 'time_usec']]\n",
    "df.index = [i for i in range(0, len(df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2117127e-a1a0-4c32-a52a-4fe6db55e8b3",
   "metadata": {},
   "source": [
    "## Partitioning of URL's\n",
    "\n",
    "As we want to separate the domain form the whole address we use advertools.url_to_df() function, which creates a dataframe object where each row gets an entry for a specific url seciton. We'll only use the netloc field, as it's the domain we look after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de5df45c-9c64-4113-b4a5-cb3009766420",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add = url_to_df(df.url)[['netloc']]\n",
    "df = df.join(df_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47da85f2-b840-4701-823f-ad5f37f282df",
   "metadata": {},
   "source": [
    "## Volume reduction\n",
    "\n",
    "In this section we reduce the overall volume of our dataset for the purpose of noise cutting. We're implementing the solutions derived at the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a4b63-2f5d-41e6-acee-6f084add2097",
   "metadata": {},
   "source": [
    "### Limiting repetitions\n",
    "\n",
    "As stated before, we're cutting short the lurking peroids, thus getting closer to representing the actual \"entrances\". We behave in line with an assumption that can't switch between multiple browser cards and we ignore situations where I could potentially change the sitetype by switching to a card opened some time ago. It will come into play later, while modeling the timeseries.\n",
    "\n",
    "I decided to treat all google search queries as noise. I share the view that it may be a bold move to think that a great deal of potential information isn't lost this way, but I justify it by saing that even if we traet each query as a separate entrance (instead of bucketing under google.com domain) each included page may greately broaden the wordspace we'll have to choose features for.\n",
    "\n",
    "Later we drop marked rows along with the temporary \"del\" marking column.\n",
    "\n",
    "It's also worth to create a table consisting of browsing history share per domain before removing repetitions for the sake of later analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f54d82f-3994-45e3-8772-3c29269a7f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amounts = df['netloc'].value_counts() #Zapisujemy ogólne ilości odwiedzin stron przed ich wywaleniem\n",
    "df['del'] = [False if (df.netloc[i] == df.netloc[i+1] or df.netloc[i] == 'www.google.com') else True\n",
    "             for i in range(0, len(df.values)-1)] + [True] #Tylko pojedyncze wpisy z bloku powtórek względem całej domeny i won z google, może nawet z translatem\n",
    "df = df[df['del']][['time_usec', 'title', 'url', 'netloc']] #Wyrzucamy kolumnę del"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c7122f-bcb0-4873-8b9c-ac188632ccd3",
   "metadata": {},
   "source": [
    "### Reduction of outliers\n",
    "\n",
    "I decided to drop all entries about sites that I've visited not more than 23 times during the last 6 months. I'd be hard to speak about a routine if I didn't visit a site at least 5 times per month, co it'd be reasonable to cut it off here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc2e4101-ccb4-482c-a323-0991471d6ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['netloc'].map(df['netloc'].value_counts()) > 23] #Redukcja stron gdzie witałem (i spędzałem posiedzenie) mniej niż 5 razy na miesiąc ~ 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18061d3d-7656-4bed-91c2-d4c7db505416",
   "metadata": {},
   "source": [
    "## Downloading\n",
    "\n",
    "Buckets dataframe is created to keep just two types of information: the domain name and a wordbag field, to which we will append the trimmed and prepared contents of its subdirectiories that I've visited. In order to cut down on the amount of iterations we create a set called history which will store full URL's of sites whose content has been already downloaded.\n",
    "\n",
    "To keep it even simpler we only allow ourselves to include sites whose response code is equal to 200 (meaning connection without exceptions). It later tourns out that few sites, ylilauta.org to give an example, is protected against webcrawling bots, so we'll have to label them manually.\n",
    "\n",
    "The process of stripping the text has three steps. Firstly we strip it out of html tags, secondly we analise each character to keep only normal letters.\n",
    "Secondly each word is getting checked for appearance of uppercase letters inside of it.\n",
    "There's a large probability that those would be leftovers of variable names from code. Additionally we specify to keep them in the length range between 4 and 20 characters, lowercasing them all afterwards.\n",
    "It's all finished by stemming with the use of two lemmatisation libraries. Morfeusz() to be used for polish and a PorterStemmer for english, usage of which is regulated by classify() function from langid library applied to first 20 words from the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef53533-aadf-4bf3-9337-c2f58cb9069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buckets = pd.DataFrame({'netloc' : df['netloc'].unique(),  #Tworzymy tablicę wiader\n",
    "                           'wordbag': ''})\n",
    "history = set()\n",
    "\n",
    "op = webdriver.ChromeOptions()\n",
    "op.add_argument('headless')\n",
    "driver = webdriver.Chrome(executable_path=CHROMEDRIVER_EXEC, options=op)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    \n",
    "    if row['url'] not in history:\n",
    "        \n",
    "        sc = 0\n",
    "        history.add(row['url'])\n",
    "\n",
    "        try:\n",
    "            page = requests.get(row['url'], verify=False)\n",
    "            sc = page.status_code\n",
    "        except: \n",
    "            pass\n",
    "        if sc == 200:\n",
    "            print(f'Conducting {row.url} ...')\n",
    "            \n",
    "            driver.get(row['url'])\n",
    "            for _ in range(10):\n",
    "                sleep(0.35)\n",
    "                driver.execute_script('return scrollBy(0, 400);')\n",
    "\n",
    "            soup =  BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            text = ''.join(i for i in soup.stripped_strings)\n",
    "\n",
    "            words_lst = ''.join(e if (e.isalnum() or e == ' ') and not e.isdigit() else ' ' #Analiza znaków\n",
    "                                for e in text).split() \n",
    "            words_lst = [e.lower() for e in words_lst #Analiza słów\n",
    "                            if not any([bool(re.match(r'.\\w*[A-Z]\\w*', e)), len(e)>20, len(e)<4])] \n",
    "\n",
    "            if langid.classify(' '.join(words_lst[:20]))[0] == 'pl':\n",
    "                stemmer = Morfeusz()\n",
    "                text = ' '.join(next(iter(stemmer.analyse(e)))[2][1].split(':')[0] for e in words_lst if not e in stopwords.words('polish')) \n",
    "            else:\n",
    "                stemmer = PorterStemmer()\n",
    "                text = ' '.join(stemmer.stem(e) for e in words_lst if not e in stopwords.words('english')) #Stemming\n",
    "\n",
    "            if row['netloc'] in df_buckets['netloc'].values:\n",
    "                df_buckets.loc[df_buckets['netloc'] == row['netloc'], 'wordbag'] += f' {text} '\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7c38eb-5f24-4d17-b5a7-8c61bb93d74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>netloc</th>\n",
       "      <th>wordbag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>www.youtube.com</td>\n",
       "      <td>wojna trwać dzień Rosjanin walczyć mariupol c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ylilauta.org</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>www.linkedin.com</td>\n",
       "      <td>zarejestrować strona trzeci klient partner do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>www.instagram.com</td>\n",
       "      <td>relacj wyświetlić telefonu nazwa użytkownika ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>www.usosweb.uj.edu.pl</td>\n",
       "      <td>piast akademik uniwersytet jagielloński menu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>login.uj.edu.pl</td>\n",
       "      <td>punkt logować uniwersytet jagielloński inform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>en.wikipedia.org</td>\n",
       "      <td>data mine wikipedia free extract discov patte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nofluffjobs.com</td>\n",
       "      <td>praca praca programista nowy oferta praca flu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>www.kaggle.com</td>\n",
       "      <td>learn python data panda tutori notebookt data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>www.overleaf.com</td>\n",
       "      <td>overleaf overleaf onlin email password incorr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pl.wikipedia.org</td>\n",
       "      <td>wariancja Wikipedia wolny edytować Wikipedia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>math.stackexchange.com</td>\n",
       "      <td>probabl expect distanc point uniformli drawn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>www.facebook.com</td>\n",
       "      <td>facebook zalogować zakres pomagać kontaktować...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>coub.com</td>\n",
       "      <td>coub biggest video meme show coub audio recog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>translate.google.pl</td>\n",
       "      <td>zanim przejść przejść wykorzystujepliki cooki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>docs.google.com</td>\n",
       "      <td>pathtracing largo pptx prezentacja włączyć pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>drive.google.com</td>\n",
       "      <td>pathtracing dysk korzystać dysk Google musieć...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>stackoverflow.com</td>\n",
       "      <td>graphic trace nois stack question overflow de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>outlook.office.com</td>\n",
       "      <td>zalogować usługi mieć konto utworzyć móc uzys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>accounts.google.com</td>\n",
       "      <td>logować konto konto usługi Google zalogować k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>github.com</td>\n",
       "      <td>releas usyd beamer theme malramsay usyd beame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>www.reddit.com</td>\n",
       "      <td>look nice beamer theme jump feed press questi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>www.geeksforgeeks.org</td>\n",
       "      <td>function server algo must topic compani avera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>www.postgresql.org</td>\n",
       "      <td>document account februari releas document sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>www.postgresqltutorial.com</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>www.w3schools.com</td>\n",
       "      <td>dark code machin data learn learn node raspbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sjp.pl</td>\n",
       "      <td>ekwilibrystyka słownik język polskie gra słow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>im.uj.edu.pl</td>\n",
       "      <td>stypendium nagroda instytut matematyka uniwer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>soundcloud.com</td>\n",
       "      <td>odkryć często odtwarzać utwór piosenka online...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>synonim.net</td>\n",
       "      <td>inaczej ciągoty słownik synonim język polskie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>www.cloudskillsboost.google</td>\n",
       "      <td>migrat premis use continu databas migrat serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>odtuclass2021f.metu.edu.tr</td>\n",
       "      <td>user access login spring semest athttp odtucl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>cytu.be</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>sjp.pwn.pl</td>\n",
       "      <td>excellence słownik język polskie język język ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>outlook.office365.com</td>\n",
       "      <td>logować móc zalogować spróbować ponownie mieć...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>app.erasmusplusols.eu</td>\n",
       "      <td>welcom onlin linguist support english danish ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>pandas.pydata.org</td>\n",
       "      <td>panda panda note stabl indexpanda columnspand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>api.stat.gov.pl</td>\n",
       "      <td>portal duży Polska baza dany gospodarka społe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>bdl.stat.gov.pl</td>\n",
       "      <td>główny urząd zostać zablokować wzgląd  główny...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>stat.gov.pl</td>\n",
       "      <td>główny urząd wykorzystujepliki cookies korzys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>www.coursera.org</td>\n",
       "      <td>middl east technic univers main contentdefaul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>www.gradescope.com</td>\n",
       "      <td>gradescop password form request demo overview...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>www.roblox.com</td>\n",
       "      <td>experiencesin peoplein avatar shopin groupsin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>www.sokmarket.com.tr</td>\n",
       "      <td>lifalif yulaf ezmesi cept uygulamaya erişebil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         netloc  \\\n",
       "0               www.youtube.com   \n",
       "1                  ylilauta.org   \n",
       "2              www.linkedin.com   \n",
       "3             www.instagram.com   \n",
       "4         www.usosweb.uj.edu.pl   \n",
       "5               login.uj.edu.pl   \n",
       "6              en.wikipedia.org   \n",
       "7               nofluffjobs.com   \n",
       "8                www.kaggle.com   \n",
       "9              www.overleaf.com   \n",
       "10             pl.wikipedia.org   \n",
       "11       math.stackexchange.com   \n",
       "12             www.facebook.com   \n",
       "13                     coub.com   \n",
       "14          translate.google.pl   \n",
       "15              docs.google.com   \n",
       "16             drive.google.com   \n",
       "17            stackoverflow.com   \n",
       "18           outlook.office.com   \n",
       "19          accounts.google.com   \n",
       "20                   github.com   \n",
       "21               www.reddit.com   \n",
       "22        www.geeksforgeeks.org   \n",
       "23           www.postgresql.org   \n",
       "24   www.postgresqltutorial.com   \n",
       "25            www.w3schools.com   \n",
       "26                       sjp.pl   \n",
       "27                 im.uj.edu.pl   \n",
       "28               soundcloud.com   \n",
       "29                  synonim.net   \n",
       "30  www.cloudskillsboost.google   \n",
       "31   odtuclass2021f.metu.edu.tr   \n",
       "32                      cytu.be   \n",
       "33                   sjp.pwn.pl   \n",
       "34        outlook.office365.com   \n",
       "35        app.erasmusplusols.eu   \n",
       "36            pandas.pydata.org   \n",
       "37              api.stat.gov.pl   \n",
       "38              bdl.stat.gov.pl   \n",
       "39                  stat.gov.pl   \n",
       "40             www.coursera.org   \n",
       "41           www.gradescope.com   \n",
       "42               www.roblox.com   \n",
       "43         www.sokmarket.com.tr   \n",
       "\n",
       "                                              wordbag  \n",
       "0    wojna trwać dzień Rosjanin walczyć mariupol c...  \n",
       "1                                                      \n",
       "2    zarejestrować strona trzeci klient partner do...  \n",
       "3    relacj wyświetlić telefonu nazwa użytkownika ...  \n",
       "4    piast akademik uniwersytet jagielloński menu ...  \n",
       "5    punkt logować uniwersytet jagielloński inform...  \n",
       "6    data mine wikipedia free extract discov patte...  \n",
       "7    praca praca programista nowy oferta praca flu...  \n",
       "8    learn python data panda tutori notebookt data...  \n",
       "9    overleaf overleaf onlin email password incorr...  \n",
       "10   wariancja Wikipedia wolny edytować Wikipedia ...  \n",
       "11   probabl expect distanc point uniformli drawn ...  \n",
       "12   facebook zalogować zakres pomagać kontaktować...  \n",
       "13   coub biggest video meme show coub audio recog...  \n",
       "14   zanim przejść przejść wykorzystujepliki cooki...  \n",
       "15   pathtracing largo pptx prezentacja włączyć pr...  \n",
       "16   pathtracing dysk korzystać dysk Google musieć...  \n",
       "17   graphic trace nois stack question overflow de...  \n",
       "18   zalogować usługi mieć konto utworzyć móc uzys...  \n",
       "19   logować konto konto usługi Google zalogować k...  \n",
       "20   releas usyd beamer theme malramsay usyd beame...  \n",
       "21   look nice beamer theme jump feed press questi...  \n",
       "22   function server algo must topic compani avera...  \n",
       "23   document account februari releas document sup...  \n",
       "24                                                     \n",
       "25   dark code machin data learn learn node raspbe...  \n",
       "26   ekwilibrystyka słownik język polskie gra słow...  \n",
       "27   stypendium nagroda instytut matematyka uniwer...  \n",
       "28   odkryć często odtwarzać utwór piosenka online...  \n",
       "29   inaczej ciągoty słownik synonim język polskie...  \n",
       "30   migrat premis use continu databas migrat serv...  \n",
       "31   user access login spring semest athttp odtucl...  \n",
       "32                                                     \n",
       "33   excellence słownik język polskie język język ...  \n",
       "34   logować móc zalogować spróbować ponownie mieć...  \n",
       "35   welcom onlin linguist support english danish ...  \n",
       "36   panda panda note stabl indexpanda columnspand...  \n",
       "37   portal duży Polska baza dany gospodarka społe...  \n",
       "38   główny urząd zostać zablokować wzgląd  główny...  \n",
       "39   główny urząd wykorzystujepliki cookies korzys...  \n",
       "40   middl east technic univers main contentdefaul...  \n",
       "41   gradescop password form request demo overview...  \n",
       "42   experiencesin peoplein avatar shopin groupsin...  \n",
       "43   lifalif yulaf ezmesi cept uygulamaya erişebil...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4800e290-5603-4ffd-906a-65e1e1d46426",
   "metadata": {},
   "source": [
    "### Saving\n",
    "\n",
    "We save it in json format for use in separate script for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "434f60f1-1bf8-458d-9e18-6789a86c3ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buckets.to_json(WORDBAGS_JSON)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
